import torch
import sys
import os

def check_gpu_detailed():
    """Verifica√ß√£o detalhada da GPU"""
    print("üîç DIAGN√ìSTICO COMPLETO DA GPU")
    print("="*50)
    
    # 1. Verificar se CUDA est√° dispon√≠vel
    print(f"‚úÖ CUDA dispon√≠vel: {torch.cuda.is_available()}")
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA n√£o est√° dispon√≠vel. Poss√≠veis causas:")
        print("   - PyTorch instalado sem suporte CUDA")
        print("   - Drivers NVIDIA n√£o instalados")
        print("   - GPU n√£o compat√≠vel")
        return False
    
    # 2. Informa√ß√µes da GPU
    print(f"üéÆ N√∫mero de GPUs: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        gpu_name = torch.cuda.get_device_name(i)
        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB
        print(f"   GPU {i}: {gpu_name} ({total_memory:.1f}GB)")
    
    # 3. GPU atual
    current_device = torch.cuda.current_device()
    print(f"üéØ GPU atual: {current_device}")
    
    # 4. Vers√µes
    print(f"üîß Vers√£o PyTorch: {torch.__version__}")
    print(f"üîß Vers√£o CUDA: {torch.version.cuda}")
    
    # 5. Teste de mem√≥ria
    try:
        allocated = torch.cuda.memory_allocated(0) / (1024**2)  # MB
        cached = torch.cuda.memory_reserved(0) / (1024**2)  # MB
        print(f"üíæ Mem√≥ria alocada: {allocated:.1f}MB")
        print(f"üíæ Mem√≥ria cache: {cached:.1f}MB")
    except Exception as e:
        print(f"‚ùå Erro ao verificar mem√≥ria: {e}")
    
    # 6. Teste pr√°tico
    print("\nüß™ TESTE PR√ÅTICO")
    print("-"*30)
    
    try:
        # Criar tensor na GPU
        test_tensor = torch.randn(1000, 1000).cuda()
        print("‚úÖ Tensor criado na GPU com sucesso")
        
        # Opera√ß√£o simples
        result = test_tensor @ test_tensor.T
        print("‚úÖ Opera√ß√£o matricial na GPU executada")
        
        # Limpar mem√≥ria
        del test_tensor, result
        torch.cuda.empty_cache()
        print("‚úÖ Mem√≥ria limpa com sucesso")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste pr√°tico: {e}")
        return False

def test_transformers_gpu():
    """Testa se os transformers conseguem usar a GPU"""
    print("\nü§ñ TESTE DOS TRANSFORMERS")
    print("-"*30)
    
    try:
        from transformers import pipeline
        
        # Criar pipeline de an√°lise de sentimentos com modelo mais seguro
        print("üì° Carregando modelo de sentimentos multil√≠ngue...")
        sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
            device=0 if torch.cuda.is_available() else -1,  # 0 = GPU, -1 = CPU
            use_safetensors=True
        )
        
        device_used = "GPU" if sentiment_pipeline.device.type == "cuda" else "CPU"
        print(f"‚úÖ Modelo carregado no dispositivo: {device_used}")
        
        # Teste de an√°lise
        test_text = "Este produto √© muito ruim e decepcionante."
        result = sentiment_pipeline(test_text)
        print(f"‚úÖ An√°lise executada: {result}")
        
        # Teste de classifica√ß√£o zero-shot
        print("\nüìä Testando classifica√ß√£o zero-shot...")
        classifier = pipeline(
            "zero-shot-classification",
            model="MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli",
            device=0 if torch.cuda.is_available() else -1,
            use_safetensors=True
        )
        
        test_labels = ["turismo", "tecnologia", "alimenta√ß√£o"]
        classification_result = classifier("Visitei Porto de Galinhas e a praia estava linda", test_labels)
        print(f"‚úÖ Classifica√ß√£o executada: {classification_result['labels'][0]} ({classification_result['scores'][0]:.3f})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erro no teste dos transformers: {e}")
          # Teste alternativo com modelo mais simples
        print("\nüîÑ Tentando modelo alternativo...")
        try:
            sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model="nlptown/bert-base-multilingual-uncased-sentiment",
                device=0 if torch.cuda.is_available() else -1
                # Removido use_safetensors=True pois este modelo j√° usa automaticamente
            )
            
            device_used = "GPU" if sentiment_pipeline.device.type == "cuda" else "CPU"
            print(f"‚úÖ Modelo alternativo carregado no dispositivo: {device_used}")
            
            test_text = "Este produto √© muito ruim e decepcionante."
            result = sentiment_pipeline(test_text)
            print(f"‚úÖ An√°lise executada: {result}")
            
            return True
            
        except Exception as e2:
            print(f"‚ùå Erro no modelo alternativo: {e2}")
            return False

if __name__ == "__main__":
    gpu_ok = check_gpu_detailed()
    
    if gpu_ok:
        transformers_ok = test_transformers_gpu()
        
        if transformers_ok:
            print("\nüéâ TUDO FUNCIONANDO!")
            print("‚úÖ GPU dispon√≠vel e funcional")
            print("‚úÖ Transformers usando GPU")
        else:
            print("\n‚ö†Ô∏è GPU OK, mas transformers com problemas")
    else:
        print("\n‚ùå Problemas com a GPU detectados")
        print("üí° Sugest√µes:")
        print("   1. Reinstalar PyTorch com CUDA")
        print("   2. Verificar drivers NVIDIA")
        print("   3. Usar CPU como alternativa")